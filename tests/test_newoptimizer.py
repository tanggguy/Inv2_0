#!/usr/bin/env python3
"""
üß™ SCRIPT DE BENCHMARK - Comparaison des Performances

Compare les performances entre l'ancienne et la nouvelle version de l'optimiseur.

Usage:
    python benchmark_optimizer.py
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

import time
from datetime import datetime
from optimization.optimizer import UnifiedOptimizer
from optimization.optimization_config import load_preset
from strategies.masuperstrategie import MaSuperStrategie


def print_header(title):
    """Affiche un header format√©"""
    print("\n" + "="*80)
    print(f"  {title}")
    print("="*80 + "\n")


def format_time(seconds):
    """Formate un temps en secondes"""
    if seconds < 60:
        return f"{seconds:.2f}s"
    else:
        minutes = int(seconds // 60)
        secs = seconds % 60
        return f"{minutes}m {secs:.1f}s"


def run_benchmark():
    """Lance le benchmark"""
    
    print_header("üß™ BENCHMARK - OPTIMISEUR OPTIMIS√â")
    
    # Configuration du test
    print("üìã Configuration du test:")
    print("  ‚Ä¢ Strat√©gie: MaSuperStrategie")
    print("  ‚Ä¢ Preset: quick")
    print("  ‚Ä¢ Type: Grid Search")
    
    # Charger la config
    config = load_preset('quick')
    
    # Afficher les d√©tails
    param_grid = config.get('param_grid', {})
    total_combos = 1
    for values in param_grid.values():
        total_combos *= len(values)
    
    print(f"  ‚Ä¢ Symboles: {', '.join(config.get('symbols', []))}")
    print(f"  ‚Ä¢ P√©riode: {config['period']['start']} ‚Üí {config['period']['end']}")
    print(f"  ‚Ä¢ Param√®tres: {param_grid}")
    print(f"  ‚Ä¢ Combinaisons totales: {total_combos}")
    
    # Pr√©parer les m√©triques
    metrics = {
        'preload_time': 0,
        'backtest_time': 0,
        'total_time': 0,
        'combinations_tested': 0,
        'best_sharpe': 0,
        'cache_used': False
    }
    
    print_header("üöÄ EX√âCUTION DU BENCHMARK")
    
    # Timer global
    global_start = time.time()
    
    # Cr√©er l'optimiseur
    print("üì¶ Cr√©ation de l'optimiseur...")
    optimizer = UnifiedOptimizer(
        MaSuperStrategie,
        config,
        optimization_type='grid_search',
        verbose=False  # Pas de logs d√©taill√©s pour le benchmark
    )
    
    print(f"‚úÖ Optimiseur cr√©√©: {optimizer.run_id}\n")
    
    # Callback de progression
    last_progress = 0
    def progress_callback(pct):
        nonlocal last_progress
        current = int(pct * 100)
        if current >= last_progress + 10:  # Afficher tous les 10%
            print(f"  Progression: {current}%")
            last_progress = current
    
    # Lancer l'optimisation
    print("‚è±Ô∏è  D√©marrage de l'optimisation...\n")
    
    try:
        results = optimizer.run(progress_callback=progress_callback)
        
        global_end = time.time()
        
        # Calculer les m√©triques
        metrics['total_time'] = global_end - global_start
        metrics['combinations_tested'] = results.get('total_combinations', 0)
        metrics['best_sharpe'] = results.get('best', {}).get('sharpe', 0)
        metrics['cache_used'] = optimizer._cache_loaded
        
        # Calculer le temps moyen par combinaison
        time_per_combo = metrics['total_time'] / metrics['combinations_tested'] if metrics['combinations_tested'] > 0 else 0
        
        # Afficher les r√©sultats
        print_header("üìä R√âSULTATS DU BENCHMARK")
        
        print("‚è±Ô∏è  Temps d'ex√©cution:")
        print(f"  ‚Ä¢ Total: {format_time(metrics['total_time'])}")
        print(f"  ‚Ä¢ Par combinaison: {time_per_combo:.3f}s")
        
        print("\nüìà Performance:")
        print(f"  ‚Ä¢ Combinaisons test√©es: {metrics['combinations_tested']}")
        print(f"  ‚Ä¢ Meilleur Sharpe: {metrics['best_sharpe']:.2f}")
        print(f"  ‚Ä¢ Cache utilis√©: {'‚úÖ Oui' if metrics['cache_used'] else '‚ùå Non'}")
        
        print("\nüèÜ Meilleurs param√®tres:")
        best = results.get('best', {})
        for key, value in best.items():
            if key not in ['sharpe', 'return', 'drawdown', 'trades', 'win_rate']:
                print(f"  ‚Ä¢ {key}: {value}")
        
        print("\nüí° M√©triques de qualit√©:")
        print(f"  ‚Ä¢ Sharpe Ratio: {best.get('sharpe', 0):.2f}")
        print(f"  ‚Ä¢ Return: {best.get('return', 0):.2f}%")
        print(f"  ‚Ä¢ Max Drawdown: {best.get('drawdown', 0):.2f}%")
        print(f"  ‚Ä¢ Nombre de trades: {best.get('trades', 0)}")
        print(f"  ‚Ä¢ Win rate: {best.get('win_rate', 0):.2f}%")
        
        # Estimation avec l'ancienne version
        print_header("üìâ COMPARAISON AVEC ANCIENNE VERSION (ESTIMATION)")
        
        # Estimer le temps de l'ancienne version
        # Hypoth√®se: 3s de chargement par combinaison + 0.6s de backtest
        old_time_estimate = (metrics['combinations_tested'] * 3.0) + (metrics['combinations_tested'] * 0.6)
        improvement = old_time_estimate / metrics['total_time'] if metrics['total_time'] > 0 else 0
        time_saved = old_time_estimate - metrics['total_time']
        
        print("‚ö†Ô∏è  Note: Estimation bas√©e sur temps de chargement de 3s/combo")
        print(f"\n  Ancienne version (estim√©e): {format_time(old_time_estimate)}")
        print(f"  Nouvelle version (mesur√©e): {format_time(metrics['total_time'])}")
        print(f"  \n  üöÄ Am√©lioration: {improvement:.1f}x plus rapide !")
        print(f"  ‚è∞ Temps √©conomis√©: {format_time(time_saved)}")
        
        # Calcul du gain par optimisation
        if improvement >= 5:
            emoji = "üî•"
            comment = "EXCELLENT gain !"
        elif improvement >= 3:
            emoji = "üéØ"
            comment = "Tr√®s bon gain !"
        elif improvement >= 2:
            emoji = "‚úÖ"
            comment = "Bon gain !"
        else:
            emoji = "‚ö†Ô∏è"
            comment = "Gain mod√©r√©"
        
        print(f"\n  {emoji} {comment}")
        
        # Projection sur gros volumes
        print_header("üîÆ PROJECTION SUR GROS VOLUMES")
        
        large_combos = 1000
        large_new_time = large_combos * time_per_combo
        large_old_time = large_combos * 3.6
        
        print(f"Pour {large_combos} combinaisons:")
        print(f"  Ancienne version: {format_time(large_old_time)}")
        print(f"  Nouvelle version: {format_time(large_new_time)}")
        print(f"  √âconomie: {format_time(large_old_time - large_new_time)}")
        
        # Sauvegarde des r√©sultats
        print_header("üíæ SAUVEGARDE")
        
        benchmark_results = {
            'timestamp': datetime.now().isoformat(),
            'run_id': optimizer.run_id,
            'config': {
                'strategy': 'MaSuperStrategie',
                'preset': 'quick',
                'symbols': config.get('symbols', []),
                'period': config['period'],
                'param_grid': param_grid
            },
            'metrics': metrics,
            'results': {
                'best_sharpe': best.get('sharpe', 0),
                'best_return': best.get('return', 0),
                'best_params': {k: v for k, v in best.items() 
                              if k not in ['sharpe', 'return', 'drawdown', 'trades', 'win_rate']}
            },
            'performance': {
                'total_time_seconds': metrics['total_time'],
                'time_per_combo_seconds': time_per_combo,
                'estimated_old_time': old_time_estimate,
                'improvement_factor': improvement
            }
        }
        
        # Sauvegarder dans un fichier
        import json
        benchmark_file = Path(__file__).parent / 'benchmark_results.json'
        with open(benchmark_file, 'w') as f:
            json.dump(benchmark_results, f, indent=2)
        
        print(f"‚úÖ R√©sultats sauvegard√©s dans: {benchmark_file}")
        
        print_header("‚úÖ BENCHMARK TERMIN√â")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå Erreur lors du benchmark: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """Fonction principale"""
    
    print("\n" + "üéØ"*40)
    print("  SCRIPT DE BENCHMARK - OPTIMISEUR")
    print("üéØ"*40)
    
    print("\nCe script va mesurer les performances de l'optimiseur optimis√©.")
    print("Il comparera √©galement avec une estimation de l'ancienne version.\n")
    
    input("Appuyez sur Entr√©e pour commencer...")
    
    success = run_benchmark()
    
    if success:
        print("\n‚úÖ Benchmark r√©ussi !")
        print("Les r√©sultats ont √©t√© sauvegard√©s dans benchmark_results.json")
    else:
        print("\n‚ùå Le benchmark a √©chou√©.")
        return 1
    
    return 0


if __name__ == "__main__":
    sys.exit(main())