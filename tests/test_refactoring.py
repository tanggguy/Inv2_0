#!/usr/bin/env python3
"""
Script de test pour valider le refactoring de l'optimisation

Teste :
1. Chargement des configs
2. Grid Search
3. Walk-Forward  
4. Stockage des rÃ©sultats
5. Historique
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

from optimization.optimization_config import OptimizationConfig, load_preset
from optimization.optimizer import UnifiedOptimizer
from optimization.results_storage import ResultsStorage
from strategies.masuperstrategie import MaSuperStrategie


def test_config():
    """Test 1: Configuration"""
    print("\n" + "="*70)
    print("TEST 1: CONFIGURATION")
    print("="*70)
    
    config_manager = OptimizationConfig()
    
    # Liste des presets
    presets = config_manager.list_presets()
    print(f"âœ“ {len(presets)} presets chargÃ©s: {', '.join(presets)}")
    
    # Charger un preset
    quick = config_manager.get_preset('quick')
    print(f"\nâœ“ Preset 'quick' chargÃ©:")
    print(config_manager.get_config_summary(quick))
    
    # Validation
    is_valid, errors = config_manager.validate_config(quick)
    assert is_valid, f"Config invalide: {errors}"
    print("\nâœ“ Validation OK")
    
    return quick


def test_grid_search(config):
    """Test 2: Grid Search"""
    print("\n" + "="*70)
    print("TEST 2: GRID SEARCH")
    print("="*70)
    
    optimizer = UnifiedOptimizer(
        MaSuperStrategie,
        config,
        optimization_type='grid_search',
        verbose=True
    )
    
    print(f"\nğŸ”¬ Lancement Grid Search...")
    print(f"   Run ID: {optimizer.run_id}")
    
    results = optimizer.run()
    
    print(f"\nâœ“ Grid Search terminÃ©")
    print(f"   Combinaisons testÃ©es: {results['total_combinations']}")
    print(f"   Meilleur Sharpe: {results['best'].get('sharpe', 0):.2f}")
    print(f"   Meilleur Return: {results['best'].get('return', 0):.2f}%")
    
    return results


def test_walk_forward():
    """Test 3: Walk-Forward"""
    print("\n" + "="*70)
    print("TEST 3: WALK-FORWARD")
    print("="*70)
    
    config = load_preset('walk_forward_quick')
    
    optimizer = UnifiedOptimizer(
        MaSuperStrategie,
        config,
        optimization_type='walk_forward',
        verbose=True
    )
    
    print(f"\nğŸš¶ Lancement Walk-Forward...")
    print(f"   Run ID: {optimizer.run_id}")
    
    results = optimizer.run()
    
    print(f"\nâœ“ Walk-Forward terminÃ©")
    if 'statistics' in results:
        stats = results['statistics']
        print(f"   Avg In-Sample Sharpe: {stats.get('avg_in_sharpe', 0):.2f}")
        print(f"   Avg Out-Sample Sharpe: {stats.get('avg_out_sharpe', 0):.2f}")
        print(f"   DÃ©gradation moyenne: {stats.get('avg_degradation', 0):.2f}")
    
    return results


def test_storage():
    """Test 4: Stockage et historique"""
    print("\n" + "="*70)
    print("TEST 4: STOCKAGE ET HISTORIQUE")
    print("="*70)
    
    storage = ResultsStorage()
    
    # Lister les runs
    runs = storage.list_runs()
    print(f"\nâœ“ {len(runs)} runs dans l'historique")
    
    if runs:
        # Afficher les 3 derniers
        print("\nğŸ“‹ Derniers runs:")
        for run in runs[-3:]:
            print(f"   â€¢ {run['run_id']}")
            print(f"     {run['strategy']} - Sharpe: {run['best_sharpe']:.2f}")
        
        # Charger un run
        last_run_id = runs[-1]['run_id']
        print(f"\nğŸ“‚ Chargement du run: {last_run_id}")
        loaded = storage.load_run(last_run_id)
        
        if loaded:
            print(f"   âœ“ Run chargÃ©:")
            print(f"     Strategy: {loaded['summary']['strategy']}")
            print(f"     Type: {loaded['summary']['optimization_type']}")
            print(f"     Best Sharpe: {loaded['summary']['best_params'].get('sharpe', 0):.2f}")
        
        # Statistiques globales
        print("\nğŸ“Š Statistiques globales:")
        stats = storage.get_statistics()
        print(f"   Total runs: {stats['total_runs']}")
        print(f"   StratÃ©gies testÃ©es: {stats['total_strategies']}")
        print(f"   Best Sharpe ever: {stats['best_sharpe']:.2f}")
        print(f"   Best Return ever: {stats['best_return']:.2f}%")
        
        # Comparaison (si au moins 2 runs)
        if len(runs) >= 2:
            print("\nğŸ” Comparaison des 2 derniers runs:")
            comparison = storage.compare_runs([runs[-2]['run_id'], runs[-1]['run_id']])
            if comparison is not None:
                print(comparison.to_string(index=False))


def test_custom_config():
    """Test 5: Configuration custom"""
    print("\n" + "="*70)
    print("TEST 5: CONFIGURATION CUSTOM")
    print("="*70)
    
    config_manager = OptimizationConfig()
    
    # CrÃ©er une config custom
    custom = config_manager.create_custom(
        symbols=['AAPL'],
        start_date='2023-06-01',
        end_date='2023-12-31',
        param_grid={
            'ma_period': [15, 25],
            'rsi_period': [14]
        },
        capital=50000,
        name="Test Custom",
        description="Config de test personnalisÃ©e"
    )
    
    print("\nâœ“ Config custom crÃ©Ã©e:")
    print(config_manager.get_config_summary(custom))
    
    # Merge avec preset
    print("\nğŸ”€ Merge avec preset 'quick':")
    merged = config_manager.merge_configs('quick', {
        'symbols': ['MSFT'],
        'capital': 150000
    })
    print(config_manager.get_config_summary(merged))
    
    return custom


def main():
    """Fonction principale de test"""
    
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘           ğŸ§ª TEST DU REFACTORING - OPTIMIZATION MODULE ğŸ§ª           â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    try:
        # Test 1: Config
        config = test_config()
        
        # Test 2: Grid Search
        grid_results = test_grid_search(config)
        
        # Test 3: Walk-Forward (optionnel - prend plus de temps)
        run_walk_forward = input("\nâ–¶ Lancer Walk-Forward (plus long) ? (o/n) [n]: ").strip().lower()
        if run_walk_forward == 'o':
            wf_results = test_walk_forward()
        
        # Test 4: Storage
        test_storage()
        
        # Test 5: Custom config
        test_custom_config()
        
        # RÃ©sumÃ© final
        print("\n" + "="*70)
        print("âœ… TOUS LES TESTS SONT PASSÃ‰S AVEC SUCCÃˆS !")
        print("="*70)
        
        print("\nğŸ“ Fichiers crÃ©Ã©s:")
        print("   â€¢ config/optimization_presets.json")
        print("   â€¢ optimization/config.py")
        print("   â€¢ optimization/optimizer.py")
        print("   â€¢ optimization/results_storage.py")
        print("   â€¢ results/history/optimization_runs.json")
        print("   â€¢ results/details/{run_id}/")
        
        print("\nğŸ¯ Prochaines Ã©tapes:")
        print("   1. âœ“ Refactoring terminÃ©")
        print("   2. â†’ CrÃ©er le dashboard Streamlit")
        print("   3. â†’ Migrer les anciens scripts si nÃ©cessaire")
        
        return 0
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Tests interrompus par l'utilisateur")
        return 1
    
    except Exception as e:
        print(f"\nâŒ ERREUR: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)