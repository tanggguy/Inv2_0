#!/usr/bin/env python3
"""
Script de test pour vÃ©rifier l'unification des param_grid
Teste que optimization_presets.json est bien la source unique pour tous les types d'optimisation
"""

import sys
from pathlib import Path

# Ajouter le rÃ©pertoire parent au path
sys.path.insert(0, str(Path(__file__).parent.parent))

from optimization.optimization_config import OptimizationConfig


def test_preset_loading():
    """Test 1: Chargement des presets"""
    print("\n" + "="*70)
    print("TEST 1: CHARGEMENT DES PRESETS")
    print("="*70)
    
    config_mgr = OptimizationConfig()
    presets = config_mgr.list_presets()
    
    print(f"âœ“ {len(presets)} presets trouvÃ©s:")
    for preset_name in presets:
        print(f"  â€¢ {preset_name}")
    
    assert len(presets) > 0, "Aucun preset trouvÃ©!"
    print("\nâœ… Test 1 RÃ‰USSI\n")


def test_grid_search_config():
    """Test 2: Configuration Grid Search"""
    print("="*70)
    print("TEST 2: CONFIGURATION GRID SEARCH")
    print("="*70)
    
    config_mgr = OptimizationConfig()
    config = config_mgr.get_preset('standard', opt_type='grid_search')
    
    print(f"\nğŸ“Š Preset 'standard' pour Grid Search:")
    print(f"  â€¢ Symboles: {config['symbols']}")
    print(f"  â€¢ PÃ©riode: {config['period']['start']} â†’ {config['period']['end']}")
    print(f"  â€¢ Capital: ${config['capital']:,}")
    
    print(f"\n  Param_grid:")
    for param, values in config['param_grid'].items():
        print(f"    â€¢ {param}: {len(values)} valeurs - {values}")
    
    assert 'param_grid' in config, "param_grid manquant!"
    assert len(config['param_grid']) > 0, "param_grid vide!"
    
    print("\nâœ… Test 2 RÃ‰USSI\n")


def test_optuna_config():
    """Test 3: Configuration Optuna"""
    print("="*70)
    print("TEST 3: CONFIGURATION OPTUNA")
    print("="*70)
    
    config_mgr = OptimizationConfig()
    config = config_mgr.get_preset('standard', opt_type='optuna')
    
    print(f"\nğŸš€ Preset 'standard' pour Optuna:")
    print(f"  â€¢ Symboles: {config['symbols']}")
    print(f"  â€¢ PÃ©riode: {config['period']['start']} â†’ {config['period']['end']}")
    
    print(f"\n  Param_grid (DOIT Ãªtre le mÃªme que Grid Search):")
    for param, values in config['param_grid'].items():
        print(f"    â€¢ {param}: {len(values)} valeurs - {values}")
    
    print(f"\n  Config Optuna:")
    optuna_cfg = config.get('optuna', {})
    print(f"    â€¢ n_trials: {optuna_cfg.get('n_trials')}")
    print(f"    â€¢ sampler: {optuna_cfg.get('sampler')}")
    print(f"    â€¢ pruner: {optuna_cfg.get('pruner')}")
    print(f"    â€¢ save_plots: {optuna_cfg.get('save_plots')}")
    
    assert 'param_grid' in config, "param_grid manquant!"
    assert 'optuna' in config, "config optuna manquante!"
    assert len(config['param_grid']) > 0, "param_grid vide!"
    
    print("\nâœ… Test 3 RÃ‰USSI\n")


def test_strategy_adaptation():
    """Test 4: Adaptation pour stratÃ©gie spÃ©cifique"""
    print("="*70)
    print("TEST 4: ADAPTATION PAR STRATÃ‰GIE")
    print("="*70)
    
    config_mgr = OptimizationConfig()
    
    # Test avec RSIStrategy
    print("\nğŸ“ˆ Test avec RSIStrategy:")
    config_rsi = config_mgr.adapt_preset_for_strategy(
        'standard', 
        'RSIStrategy',
        opt_type='optuna'
    )
    
    print(f"\n  Param_grid adaptÃ© pour RSIStrategy:")
    for param, values in config_rsi['param_grid'].items():
        print(f"    â€¢ {param}: {len(values)} valeurs - {values}")
    
    # VÃ©rifier que les paramÃ¨tres sont bien ceux de RSI
    expected_params = ['rsi_period', 'oversold', 'overbought', 'stop_loss_pct']
    actual_params = list(config_rsi['param_grid'].keys())
    
    print(f"\n  ParamÃ¨tres attendus: {expected_params}")
    print(f"  ParamÃ¨tres obtenus:  {actual_params}")
    
    # Test avec MovingAverageStrategy
    print("\nğŸ“ˆ Test avec MovingAverageStrategy:")
    config_ma = config_mgr.adapt_preset_for_strategy(
        'standard',
        'MovingAverageStrategy',
        opt_type='optuna'
    )
    
    print(f"\n  Param_grid adaptÃ© pour MovingAverageStrategy:")
    for param, values in config_ma['param_grid'].items():
        print(f"    â€¢ {param}: {len(values)} valeurs - {values}")
    
    expected_params_ma = ['fast_period', 'slow_period', 'stop_loss_pct', 'take_profit_pct']
    actual_params_ma = list(config_ma['param_grid'].keys())
    
    print(f"\n  ParamÃ¨tres attendus: {expected_params_ma}")
    print(f"  ParamÃ¨tres obtenus:  {actual_params_ma}")
    
    assert config_rsi['param_grid'] != config_ma['param_grid'], \
        "Les param_grid devraient Ãªtre diffÃ©rents pour chaque stratÃ©gie!"
    
    print("\nâœ… Test 4 RÃ‰USSI\n")


def test_param_grid_consistency():
    """Test 5: CohÃ©rence param_grid Grid Search vs Optuna"""
    print("="*70)
    print("TEST 5: COHÃ‰RENCE PARAM_GRID (Grid Search vs Optuna)")
    print("="*70)
    
    config_mgr = OptimizationConfig()
    
    # Charger le mÃªme preset pour les deux types
    config_grid = config_mgr.get_preset('standard', opt_type='grid_search')
    config_optuna = config_mgr.get_preset('standard', opt_type='optuna')
    
    print("\nğŸ” Comparaison des param_grid:")
    print(f"\n  Grid Search:")
    for param, values in config_grid['param_grid'].items():
        print(f"    â€¢ {param}: {values}")
    
    print(f"\n  Optuna:")
    for param, values in config_optuna['param_grid'].items():
        print(f"    â€¢ {param}: {values}")
    
    # VÃ©rifier que les param_grid sont identiques
    assert config_grid['param_grid'] == config_optuna['param_grid'], \
        "âŒ ERREUR: Les param_grid devraient Ãªtre identiques!"
    
    print("\nâœ“ Les param_grid sont identiques pour Grid Search et Optuna")
    print("âœ… Test 5 RÃ‰USSI\n")


def test_config_summary():
    """Test 6: GÃ©nÃ©ration de rÃ©sumÃ©"""
    print("="*70)
    print("TEST 6: GÃ‰NÃ‰RATION DE RÃ‰SUMÃ‰")
    print("="*70)
    
    config_mgr = OptimizationConfig()
    config = config_mgr.get_preset('standard', opt_type='optuna')
    
    summary = config_mgr.get_config_summary(config)
    print(f"\n{summary}")
    
    assert len(summary) > 0, "RÃ©sumÃ© vide!"
    print("\nâœ… Test 6 RÃ‰USSI\n")


def test_validation():
    """Test 7: Validation de configuration"""
    print("="*70)
    print("TEST 7: VALIDATION DE CONFIGURATION")
    print("="*70)
    
    config_mgr = OptimizationConfig()
    
    # Config valide
    config_valid = config_mgr.get_preset('standard')
    is_valid, errors = config_mgr.validate_config(config_valid)
    
    print(f"\nâœ“ Config valide: {is_valid}")
    if errors:
        print(f"  Erreurs: {errors}")
    
    assert is_valid, "Le preset 'standard' devrait Ãªtre valide!"
    
    # Config invalide
    config_invalid = {
        'symbols': [],  # Vide
        'period': {'start': '2023-01-01'},  # 'end' manquant
        'param_grid': {}  # Vide
    }
    
    is_valid, errors = config_mgr.validate_config(config_invalid)
    
    print(f"\nâœ“ Config invalide dÃ©tectÃ©e: {not is_valid}")
    print(f"  Erreurs trouvÃ©es:")
    for error in errors:
        print(f"    â€¢ {error}")
    
    assert not is_valid, "La config invalide devrait Ãªtre dÃ©tectÃ©e!"
    assert len(errors) > 0, "Des erreurs devraient Ãªtre trouvÃ©es!"
    
    print("\nâœ… Test 7 RÃ‰USSI\n")


def run_all_tests():
    """ExÃ©cute tous les tests"""
    print("\n" + "ğŸ§ª " + "="*68)
    print("ğŸ§ª TEST DE L'UNIFICATION DES PARAM_GRID")
    print("ğŸ§ª " + "="*68 + "\n")
    
    tests = [
        ("Chargement des presets", test_preset_loading),
        ("Configuration Grid Search", test_grid_search_config),
        ("Configuration Optuna", test_optuna_config),
        ("Adaptation par stratÃ©gie", test_strategy_adaptation),
        ("CohÃ©rence param_grid", test_param_grid_consistency),
        ("GÃ©nÃ©ration de rÃ©sumÃ©", test_config_summary),
        ("Validation de config", test_validation),
    ]
    
    passed = 0
    failed = 0
    
    for test_name, test_func in tests:
        try:
            test_func()
            passed += 1
        except Exception as e:
            failed += 1
            print(f"\nâŒ Test '{test_name}' Ã‰CHOUÃ‰:")
            print(f"   Erreur: {e}\n")
            import traceback
            traceback.print_exc()
    
    # RÃ©sumÃ©
    print("\n" + "="*70)
    print("ğŸ“Š RÃ‰SUMÃ‰ DES TESTS")
    print("="*70)
    print(f"\nâœ… Tests rÃ©ussis: {passed}/{len(tests)}")
    print(f"âŒ Tests Ã©chouÃ©s:  {failed}/{len(tests)}")
    
    if failed == 0:
        print("\nğŸ‰ TOUS LES TESTS SONT PASSÃ‰S!")
        print("\nâœ“ L'unification des param_grid fonctionne correctement")
        print("âœ“ optimization_presets.json est bien la source unique")
        print("âœ“ Les param_grid sont cohÃ©rents entre Grid Search et Optuna")
        print("âœ“ L'adaptation par stratÃ©gie fonctionne")
    else:
        print("\nâš ï¸  CERTAINS TESTS ONT Ã‰CHOUÃ‰")
        print("   VÃ©rifiez les erreurs ci-dessus")
    
    print("\n" + "="*70 + "\n")
    
    return failed == 0


if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)